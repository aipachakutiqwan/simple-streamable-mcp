{
  "2306.08474v1": {
    "title": "Measurement-based Channel Characterization for A2A and A2G Wireless Drone Communication Systems",
    "authors": [
      "Ubeydullah Erdemir",
      "Batuhan Kaplan",
      "\u0130brahim H\u00f6kelek",
      "Ali G\u00f6r\u00e7in",
      "Hakan Ali \u00c7\u0131rpan"
    ],
    "summary": "This paper presents field measurement-based channel characterization for\nair--to--ground (A2G) and air--to--air (A2A) wireless communication systems\nusing two drones equipped with lightweight software-defined radios. A\ncorrelation-based channel sounder is employed such that the transmitting drone\nbroadcasts the sounding waveform with a pseudo-noise sequence and the receiving\ndrone captures the sounding waveform together with the location information for\nthe post-processing analysis. The path loss results demonstrate that the\nmeasurement and flat-earth two-ray results have similar trends for A2G while\nthe measurement and free space path loss are similar to each other for A2A. The\ntime delays between the direct path and multipath components are widely spread\nfor A2A while the multipath components are mostly concentrated around the\ndirect path for A2G generating a more challenging communication environment. We\nobserve that the reflections from several buildings having metal roofs and\ncladdings on the measurement site cause sudden peaks in the root-mean-square\ndelay spread. The results indicate that the A2A channel has better\ncharacteristics than the A2G under similar mobility conditions.",
    "pdf_url": "http://arxiv.org/pdf/2306.08474v1",
    "published": "2023-06-14"
  },
  "2412.14873v2": {
    "title": "Zero-Shot Artifact2Artifact: Self-incentive artifact removal for photoacoustic imaging without any data",
    "authors": [
      "Shuang Li",
      "Qian Chen",
      "Chulhong Kim",
      "Seongwook Choi",
      "Yibing Wang",
      "Yu Zhang",
      "Changhui Li"
    ],
    "summary": "Photoacoustic imaging (PAI) uniquely combines optical contrast with the\npenetration depth of ultrasound, making it critical for clinical applications.\nHowever, the quality of 3D PAI is often degraded due to reconstruction\nartifacts caused by the sparse and angle-limited configuration of detector\narrays. Existing iterative or deep learning-based methods are either\ntime-consuming or require large training datasets, significantly limiting their\npractical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a\nzero-shot self-supervised artifact removal method based on a super-lightweight\nnetwork, which leverages the fact that reconstruction artifacts are sensitive\nto irregularities caused by data loss. By introducing random perturbations to\nthe acquired PA data, it spontaneously generates subset data, which in turn\nstimulates the network to learn the artifact patterns in the reconstruction\nresults, thus enabling zero-shot artifact removal. This approach requires\nneither training data nor prior knowledge of the artifacts, and is capable of\nartifact removal for 3D PAI. For maximum amplitude projection (MAP) images or\nslice images in 3D PAI acquired with arbitrarily sparse or angle-limited\ndetector arrays, ZS-A2A employs a self-incentive strategy to complete artifact\nremoval and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in\nboth simulation study and $ in\\ vivo $ animal experiments. Results demonstrate\nthat ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing\nzero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from\n17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in\nthe following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.",
    "pdf_url": "http://arxiv.org/pdf/2412.14873v2",
    "published": "2024-12-19"
  },
  "2504.16902v2": {
    "title": "Building A Secure Agentic AI Application Leveraging A2A Protocol",
    "authors": [
      "Idan Habler",
      "Ken Huang",
      "Vineeth Sai Narajala",
      "Prashant Kulkarni"
    ],
    "summary": "As Agentic AI systems evolve from basic workflows to complex multi agent\ncollaboration, robust protocols such as Google's Agent2Agent (A2A) become\nessential enablers. To foster secure adoption and ensure the reliability of\nthese complex interactions, understanding the secure implementation of A2A is\nessential. This paper addresses this goal by providing a comprehensive security\nanalysis centered on the A2A protocol. We examine its fundamental elements and\noperational dynamics, situating it within the framework of agent communication\ndevelopment. Utilizing the MAESTRO framework, specifically designed for AI\nrisks, we apply proactive threat modeling to assess potential security issues\nin A2A deployments, focusing on aspects such as Agent Card management, task\nexecution integrity, and authentication methodologies.\n  Based on these insights, we recommend practical secure development\nmethodologies and architectural best practices designed to build resilient and\neffective A2A systems. Our analysis also explores how the synergy between A2A\nand the Model Context Protocol (MCP) can further enhance secure\ninteroperability. This paper equips developers and architects with the\nknowledge and practical guidance needed to confidently leverage the A2A\nprotocol for building robust and secure next generation agentic applications.",
    "pdf_url": "http://arxiv.org/pdf/2504.16902v2",
    "published": "2025-04-23"
  },
  "2505.17655v1": {
    "title": "Audio-to-Audio Emotion Conversion With Pitch And Duration Style Transfer",
    "authors": [
      "Soumya Dutta",
      "Avni Jain",
      "Sriram Ganapathy"
    ],
    "summary": "Given a pair of source and reference speech recordings, audio-to-audio (A2A)\nstyle transfer involves the generation of an output speech that mimics the\nstyle characteristics of the reference while preserving the content and speaker\nattributes of the source. In this paper, we propose a novel framework, termed\nas A2A Zero-shot Emotion Style Transfer (A2A-ZEST), that enables the transfer\nof reference emotional attributes to the source while retaining its speaker and\nspeech contents. The A2A-ZEST framework consists of an analysis-synthesis\npipeline, where the analysis module decomposes speech into semantic tokens,\nspeaker representations, and emotion embeddings. Using these representations, a\npitch contour estimator and a duration predictor are learned. Further, a\nsynthesis module is designed to generate speech based on the input\nrepresentations and the derived factors. This entire paradigm of\nanalysis-synthesis is trained purely in a self-supervised manner with an\nauto-encoding loss. For A2A emotion style transfer, the emotion embedding\nextracted from the reference speech along with the rest of the representations\nfrom the source speech are used in the synthesis module to generate the style\ntranslated speech. In our experiments, we evaluate the converted speech on\ncontent/speaker preservation (w.r.t. source) as well as on the effectiveness of\nthe emotion style transfer (w.r.t. reference). The proposal, A2A-ZEST, is shown\nto improve over other prior works on these evaluations, thereby enabling style\ntransfer without any parallel training data. We also illustrate the application\nof the proposed work for data augmentation in emotion recognition tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17655v1",
    "published": "2025-05-23"
  },
  "2506.01804v2": {
    "title": "A Study on the MCP x A2A Framework for Enhancing Interoperability of LLM-based Autonomous Agents",
    "authors": [
      "Cheonsu Jeong"
    ],
    "summary": "This paper provides an in-depth technical analysis and implementation\nmethodology of the open-source Agent-to-Agent (A2A) protocol developed by\nGoogle and the Model Context Protocol (MCP) introduced by Anthropic. While the\nevolution of LLM-based autonomous agents is rapidly accelerating, efficient\ninteractions among these agents and their integration with external systems\nremain significant challenges. In modern AI systems, collaboration between\nautonomous agents and integration with external tools have become essential\nelements for building practical AI applications. A2A offers a standardized\ncommunication method that enables agents developed in heterogeneous\nenvironments to collaborate effectively, while MCP provides a structured I/O\nframework for agents to connect with external tools and resources. Prior\nstudies have focused primarily on the features and applications of either A2A\nor MCP individually. In contrast, this study takes an integrated approach,\nexploring how the two protocols can complement each other to address\ninteroperability issues and facilitate efficient collaboration within complex\nagent ecosystems.",
    "pdf_url": "http://arxiv.org/pdf/2506.01804v2",
    "published": "2025-06-02"
  }
}